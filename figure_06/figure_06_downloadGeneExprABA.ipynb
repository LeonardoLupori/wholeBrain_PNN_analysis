{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "# Make a query to the API via a URL.\n",
    "def QueryAPI(url,verbose=False):\n",
    "    start_row = 0\n",
    "    num_rows = 2000\n",
    "    total_rows = -1\n",
    "    rows = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        pagedUrl = url + '&start_row=%d&num_rows=%d' % (start_row,num_rows)\n",
    "        \n",
    "        if verbose:\n",
    "            print(pagedUrl)\n",
    "            \n",
    "        source = urllib.request.urlopen(pagedUrl).read()\n",
    "\n",
    "        response = json.loads(source)\n",
    "        rows += response['msg']\n",
    "        \n",
    "        if total_rows < 0:\n",
    "            total_rows = int(response['total_rows'])\n",
    "\n",
    "        start_row += len(response['msg'])\n",
    "\n",
    "        if start_row >= total_rows:\n",
    "            done = True\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the SectionDataSets informations (~1minute)\n",
    "Roughly 22000 datasets, for ISH data both Coronal and Sagittal.\n",
    "\n",
    "Information on the genes analyzed in each specific SectionDataSet is retrieved with the &include+genes query and is stored in the column genes of the resulting dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_ID = 1\n",
    "product_ID = 1\n",
    "\n",
    "BASE = \"http://api.brain-map.org/api/v2/data\"\n",
    "target = \"/SectionDataSet/query.json?\"\n",
    "criteria = \"criteria=[failed$eq'false'][expression$eq'true'],\"\n",
    "products = \"products[id$eq{}]\".format(product_ID)\n",
    "inclusions = \"&include=genes\"\n",
    "exclusions = \"&except=blue_channel,delegate,expression,failed,failed_facet,green_channel,\" + \\\n",
    "                \"name,qc_date,red_channel,rnaseq_design_id,sphinx_id,storage_directory,weight\"\n",
    "\n",
    "# url = BASE + target + criteria + products + inclusions + exclusions\n",
    "url = BASE + target + criteria + products + inclusions + exclusions\n",
    "\n",
    "result = QueryAPI(url,verbose=False)\n",
    "sDataSet = pd.DataFrame(result)\n",
    "sDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDataSet['genes'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save daframe containing information about genes and sectionDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dataframe with the gene description for each sectionDataSet\n",
    "genesDf = pd.DataFrame([x[0] for x in sDataSet['genes']])\n",
    "\n",
    "genesDf.index = sDataSet['id']\n",
    "genesDf.index.name = 'sectionDataSet_id'\n",
    "\n",
    "genesDf = genesDf[['id','acronym','original_name','original_symbol','genomic_reference_update_id','alias_tags','entrez_id']]\n",
    "genesDf.rename(columns={'id':'gene_id'}, inplace=True)\n",
    "\n",
    "genesDf.to_csv('genesDataFrame.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup the dataframe\n",
    "Drop data concerning gene details: only retain gene_id-sectionDataset_id mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gene dictionary such tah you can access genes \n",
    "# information by searching for the id as a key\n",
    "genesDict = dict([(x[0]['id'], x) for x in sDataSet.genes])\n",
    "\n",
    "# Create explicit lists for useful infos of the genes\n",
    "g_id = [x[0]['id'] for x in sDataSet.genes]\n",
    "\n",
    "sDataSet['gene_id'] = g_id\n",
    "sDataSet.set_index('id', inplace = True)\n",
    "sDataSet.index.name = 'sectionDataset_id'\n",
    "sDataSet.drop(columns=['genes','section_thickness'],inplace=True)\n",
    "sDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download set of 316 mid-ontology level brain structures\n",
    "\n",
    "from allensdk.core.reference_space_cache import ReferenceSpaceCache\n",
    "\n",
    "# -------------------------------------------\n",
    "reference_space_key = 'annotation/ccf_2017'\n",
    "resolution = 25\n",
    "# -------------------------------------------\n",
    "\n",
    "# Create a reference space object\n",
    "rspc = ReferenceSpaceCache(resolution, reference_space_key, manifest='manifest.json')\n",
    "# ID 1 is the adult mouse structure graph\n",
    "tree = rspc.get_structure_tree(structure_graph_id=1) \n",
    "\n",
    "chosenID = 167587189 # \n",
    "\n",
    "structureList = tree.get_structures_by_set_id([chosenID])\n",
    "strDf = pd.DataFrame(structureList)\n",
    "strDf.sort_values(by='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download expression data (~9 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"http://api.brain-map.org/api/v2/data\"\n",
    "target = \"/StructureUnionize/query.json\"\n",
    "criteria = \"?criteria=structure[graph_id$eq{}]\".format(graph_ID)\n",
    "# varOfInterest = \"expression_energy\"\n",
    "\n",
    "# Initialize the expression DataFrame\n",
    "expressionDf1 = pd.DataFrame()\n",
    "expressionDf1['gene_id'] = []\n",
    "\n",
    "expressionDf2 = pd.DataFrame()\n",
    "expressionDf2['gene_id'] = []\n",
    "\n",
    "expressionDf3 = pd.DataFrame()\n",
    "expressionDf3['gene_id'] = []\n",
    "for area in strDf['id']:\n",
    "    expressionDf1[area] = []\n",
    "    expressionDf2[area] = []\n",
    "    expressionDf3[area] = []\n",
    "    \n",
    "for idx, dataSetID in enumerate(sDataSet.index.tolist()):\n",
    "\n",
    "    # Assigne the geneID for the current experiment to this row\n",
    "    expressionDf1.at[idx,'gene_id'] = int(sDataSet['gene_id'].iloc[idx])\n",
    "    expressionDf2.at[idx,'gene_id'] = int(sDataSet['gene_id'].iloc[idx])\n",
    "    expressionDf3.at[idx,'gene_id'] = int(sDataSet['gene_id'].iloc[idx])\n",
    "\n",
    "    # Download expression values for all the areas\n",
    "    urlDataSet = \",[section_data_set_id$eq{}]\".format(dataSetID)\n",
    "    print(\"Now downloading dataset #{} ({}/{})...\".format(dataSetID,idx+1,len(sDataSet.index.tolist())))\n",
    "    \n",
    "    # Query data from the API\n",
    "    url = BASE + target + criteria + urlDataSet\n",
    "    result = QueryAPI(url,verbose=False)\n",
    "    \n",
    "    # Create an addressable dict of all the areas for the current structureDataSet\n",
    "    areasDict = dict([(x['structure_id'],x) for x in result])\n",
    "    \n",
    "    # Cycle through all the aligned 316 areas.\n",
    "    # If there is expression data for one of these areas, put it in the table\n",
    "    # otherwise, put a NaN\n",
    "    for areaID in strDf['id']:\n",
    "        if areaID in areasDict.keys():\n",
    "            energy = areasDict[areaID]['expression_energy']\n",
    "            density = areasDict[areaID]['expression_density']\n",
    "            if  areasDict[areaID]['sum_expressing_pixels'] !=0:\n",
    "                intensity =  areasDict[areaID]['sum_expressing_pixel_intensity']/ areasDict[areaID]['sum_expressing_pixels']\n",
    "            else:\n",
    "                intensity =  np.nan\n",
    "        else:\n",
    "            energy = np.nan\n",
    "            density = np.nan\n",
    "            intensity = np.nan\n",
    "        expressionDf1.at[idx,areaID] = energy\n",
    "        expressionDf2.at[idx,areaID] = density\n",
    "        expressionDf3.at[idx,areaID] = intensity\n",
    "\n",
    "# expressionDf\n",
    "# Save data as a CSV\n",
    "expressionDf1.set_index('gene_id', inplace=True)\n",
    "expressionDf2.set_index('gene_id', inplace=True)\n",
    "expressionDf3.set_index('gene_id', inplace=True)\n",
    "expressionDf1.to_csv('gene_expression_ABA_energy_raw.csv')\n",
    "expressionDf2.to_csv('gene_expression_ABA_density_raw.csv')\n",
    "expressionDf3.to_csv('gene_expression_ABA_intensity_raw.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = expressionDf1.groupby(by = expressionDf1.index).mean()\n",
    "exp2 = expressionDf2.groupby(by = expressionDf2.index).mean()\n",
    "exp3 = expressionDf3.groupby(by = expressionDf3.index).mean()\n",
    "\n",
    "exp1.to_csv('gene_expression_ABA_energy.csv')\n",
    "exp2.to_csv('gene_expression_ABA_density.csv')\n",
    "exp3.to_csv('gene_expression_ABA_intensity.csv')\n",
    "\n",
    "exp1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('allen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b41230a79c73660caffd1be412225afb5bf94923aa39d73f52f6e176d33f811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
